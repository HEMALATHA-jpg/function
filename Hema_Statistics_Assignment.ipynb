{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyMYszK39jhpCPWTIBeRSu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HEMALATHA-jpg/function/blob/main/Hema_Statistics_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Explain the properties of the F-distribution.\n",
        "\n",
        "ANS--> The F-distribution, named after Sir Ronald Fisher, is a continuous probability distribution that arises frequently in statistical inference and hypothesis testing. Here are its key properties:\n",
        "\n",
        "Key Properties\n",
        "\n",
        "1. Non-negative: The F-distribution takes values ≥ 0.\n",
        "2. Right-skewed: The distribution is skewed to the right, with a longer tail.\n",
        "3. Continuous: The F-distribution is a continuous distribution.\n",
        "4. Two parameters: The F-distribution has two parameters: degrees of freedom (df1 and df2).\n",
        "5. Asymmetric: The distribution is asymmetric around its mean.\n",
        "\n",
        "Parameterization\n",
        "\n",
        "1. df1 (numerator degrees of freedom): Associated with the numerator of the F-statistic.\n",
        "2. df2 (denominator degrees of freedom): Associated with the denominator of the F-statistic.\n",
        "\n",
        "Important Properties\n",
        "\n",
        "1. Mean: The mean of the F-distribution is df2 / (df2 - 2) for df2 > 2.\n",
        "2. Variance: The variance is 2 * (df2^2 * (df1 + df2 - 2)) / ((df1 * (df2 - 2)^2 * (df2 - 4))) for df2 > 4.\n",
        "3. Mode: The mode is approximately (df1 - 2) / df1.\n",
        "4. Median: The median is approximately 1 for large df1 and df2.\n",
        "\n",
        "Special Cases\n",
        "\n",
        "1. F(1,1): Equivalent to the square of the standard normal distribution.\n",
        "2. F(1,df2): Equivalent to the square of the t-distribution with df2 degrees of freedom.\n",
        "\n",
        "Applications\n",
        "\n",
        "1. Analysis of Variance (ANOVA): F-distribution is used to test hypotheses about means.\n",
        "2. Regression analysis: F-distribution is used to test hypotheses about regression coefficients.\n",
        "3. Hypothesis testing: F-distribution is used to test hypotheses about variances."
      ],
      "metadata": {
        "id": "l9yv_DJDhC9P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?\n",
        "\n",
        "The F-distribution is used in various statistical tests, primarily for:\n",
        "\n",
        "Hypothesis Testing\n",
        "\n",
        "1. Analysis of Variance (ANOVA): F-distribution is used to test differences in means among three or more groups.\n",
        "2. Regression analysis: F-distribution is used to test the significance of regression coefficients.\n",
        "3. Variance ratio tests: F-distribution is used to compare variances between two populations.\n",
        "\n",
        "Specific Statistical Tests\n",
        "\n",
        "1. F-test for equality of variances: Tests if two populations have equal variances.\n",
        "2. F-test for regression coefficients: Tests the significance of individual regression coefficients.\n",
        "3. F-test for overall regression significance: Tests the overall significance of a regression model.\n",
        "4. Two-way ANOVA: Tests interactions between two factors.\n",
        "\n",
        "Why F-Distribution is Appropriate\n",
        "\n",
        "1. Ratio of variances: The F-distribution models the ratio of two variances, making it suitable for tests involving variance comparisons.\n",
        "2. Non-normality robustness: The F-distribution is relatively robust to non-normality, making it suitable for tests with non-normal data.\n",
        "3. Flexibility: The F-distribution accommodates various degrees of freedom, allowing for tests with different sample sizes.\n",
        "4. Asymmetry: The F-distribution's asymmetry reflects the skewed nature of variance ratios.\n",
        "\n",
        "Assumptions\n",
        "\n",
        "For valid F-distribution-based tests:\n",
        "\n",
        "1. Normality: Data should be normally distributed.\n",
        "2. Independence: Observations should be independent.\n",
        "3. Homoscedasticity: Variances should be equal across groups.\n",
        "4. Random sampling: Samples should be randomly selected.\n",
        "\n",
        "Common Software Implementation\n",
        "\n",
        "1. R: aov(), lm(), var.test()\n",
        "2. Python: scipy.stats.f, statsmodels\n",
        "3. SAS: PROC ANOVA, PROC REG\n",
        "4. SPSS: ANOVA, REGRESSION"
      ],
      "metadata": {
        "id": "1tBWMBhZhP-E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. What are the key assumptions required for conducting an F-test to compare the variances of two\n",
        "populations?\n",
        "\n",
        "ANS--> To conduct an F-test for comparing variances, the following key assumptions must be met:\n",
        "\n",
        "Statistical Assumptions\n",
        "\n",
        "1. Normality: Data from both populations should follow a normal distribution.\n",
        "2. Independence: Observations should be independent within and between samples.\n",
        "3. Homoscedasticity: Variances should be constant across all levels of the independent variable.\n",
        "4. Random sampling: Samples should be randomly selected from the populations.\n",
        "\n",
        "Population Assumptions\n",
        "\n",
        "1. Equal population distributions: Both populations should have the same shape and distribution.\n",
        "2. No outliers: Data should not contain significant outliers.\n",
        "\n",
        "Test-Specific Assumptions\n",
        "\n",
        "1. Sample sizes: Sample sizes should be sufficient (typically > 30).\n",
        "2. No significant skewness: Data should not exhibit significant skewness.\n",
        "\n",
        "F-Test Variations\n",
        "\n",
        "For specific F-test variations, additional assumptions may apply:\n",
        "\n",
        "1. F-test for equality of variances: Assumes equal population means.\n",
        "2. Levene's test: Relaxes normality assumption, but requires equal population medians.\n",
        "3. Brown-Forsythe test: Relaxes normality assumption, but requires equal population medians.\n",
        "\n",
        "Violation Consequences\n",
        "\n",
        "Violating these assumptions can lead to:\n",
        "\n",
        "1. Incorrect p-values\n",
        "2. Reduced test power\n",
        "3. Increased Type I error rate\n",
        "4. Incorrect conclusions\n",
        "\n",
        "Remedies\n",
        "\n",
        "If assumptions are violated:\n",
        "\n",
        "1. Transform data (e.g., logarithmic)\n",
        "2. Use non-parametric alternatives (e.g., Levene's test)\n",
        "3. Increase sample size\n",
        "4. Use robust statistical methods"
      ],
      "metadata": {
        "id": "CZUdhXZ3hjxW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. What is the purpose of ANOVA, and how does it differ from a t-test?\n",
        "\n",
        "ANS--> ANOVA (Analysis of Variance) and t-tests are statistical methods used to compare means, but they serve distinct purposes:\n",
        "\n",
        "Purpose of ANOVA\n",
        "\n",
        "1. Compare means among three or more groups: ANOVA determines if there's a significant difference between the means of three or more independent groups.\n",
        "2. Identify significant differences: ANOVA detects if the variation between groups is greater than the variation within groups.\n",
        "3. Analyze multiple factors: ANOVA can examine the interaction between multiple independent variables.\n",
        "\n",
        "Purpose of t-test\n",
        "\n",
        "1. Compare means between two groups: t-tests determine if there's a significant difference between the means of two independent groups.\n",
        "2. Examine pairwise differences: t-tests are used for pairwise comparisons.\n",
        "\n",
        "Key differences\n",
        "\n",
        "1. Number of groups: ANOVA (three or more groups) vs. t-test (two groups)\n",
        "2. Comparison type: ANOVA (overall comparison) vs. t-test (pairwise comparison)\n",
        "3. Assumptions: ANOVA requires homogeneity of variances, normality, and independence, while t-tests require normality and independence\n",
        "4. Statistical power: ANOVA is more powerful than multiple t-tests for comparing multiple groups\n",
        "\n",
        "Types of ANOVA\n",
        "\n",
        "1. One-way ANOVA: Compares means among three or more groups with one independent variable.\n",
        "2. Two-way ANOVA: Examines interactions between two independent variables.\n",
        "3. Repeated Measures ANOVA: Analyzes data from repeated measurements."
      ],
      "metadata": {
        "id": "hxZGozzQh4Co"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more\n",
        "than two groups.\n",
        "\n",
        "ANS--> Here's when and why to use one-way ANOVA instead of multiple t-tests:\n",
        "\n",
        "When to Use One-Way ANOVA\n",
        "\n",
        "1. Comparing three or more groups: One-way ANOVA is suitable for comparing means among three or more independent groups.\n",
        "2. Examining overall differences: When you want to determine if there's an overall significant difference among groups, without specifying which groups differ.\n",
        "3. Multiple comparisons: When comparing more than two groups, one-way ANOVA reduces the number of comparisons needed.\n",
        "\n",
        "Why Not Multiple t-tests\n",
        "\n",
        "1. Increased Type I error rate: Conducting multiple t-tests increases the likelihood of false positives (Type I errors).\n",
        "2. Inflated alpha level: Multiple comparisons inflate the overall alpha level (typically 0.05), leading to incorrect conclusions.\n",
        "3. Lack of overall significance: Multiple t-tests don't provide an overall significance test, making it difficult to determine if differences are due to chance.\n",
        "4. Multiple comparisons problem: Multiple t-tests don't account for the number of comparisons, leading to an increased risk of false discoveries.\n",
        "\n",
        "Advantages of One-Way ANOVA\n",
        "\n",
        "1. Controlled Type I error rate: One-way ANOVA maintains the specified alpha level (typically 0.05).\n",
        "2. Overall significance test: One-way ANOVA provides an overall significance test, indicating whether differences among groups are statistically significant.\n",
        "3. Reduced number of comparisons: One-way ANOVA eliminates the need for multiple pairwise comparisons.\n",
        "4. Increased statistical power: One-way ANOVA is more powerful than multiple t-tests for detecting overall differences.\n",
        "\n",
        "Assumptions of One-Way ANOVA\n",
        "\n",
        "1. Normality: Data should be normally distributed within each group.\n",
        "2. Homoscedasticity: Variances should be equal across groups.\n",
        "3. Independence: Observations should be independent within and between groups.\n",
        "\n",
        "Post-Hoc Tests\n",
        "\n",
        "After significant one-way ANOVA results, use post-hoc tests (e.g., Tukey's HSD, Scheffé test) to:\n",
        "\n",
        "1. Identify specific group differences.\n",
        "2. Control for multiple comparisons."
      ],
      "metadata": {
        "id": "Bsv3aKO1iJdi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. Explain how variance is partitioned in ANOVA into between-group variance and within-group variance.\n",
        "How does this partitioning contribute to the calculation of the F-statistic?\n",
        "\n",
        "ANS--> In ANOVA, variance is partitioned into two components:\n",
        "\n",
        "Between-Group Variance (SSB)\n",
        "\n",
        "1. Measures variation between group means: Differences between group means and the overall mean.\n",
        "2. Reflects systematic differences: Variance attributed to the independent variable (factor).\n",
        "3. Calculated as: SSB = Σn_i(μ_i - μ)^2, where n_i is group size, μ_i is group mean, and μ is overall mean.\n",
        "\n",
        "Within-Group Variance (SSW)\n",
        "\n",
        "1. Measures variation within groups: Differences between individual observations and their group means.\n",
        "2. Reflects random error: Variance attributed to chance or individual differences.\n",
        "3. Calculated as: SSW = ΣΣ(x_ij - μ_i)^2, where x_ij is individual observation.\n",
        "\n",
        "Total Variance (SST)\n",
        "\n",
        "1. Sum of SSB and SSW: SST = SSB + SSW.\n",
        "2. Measures overall variation: Total variance in the data.\n",
        "\n",
        "Mean Square (MS) Calculation\n",
        "\n",
        "1. MS Between (MSB): MSB = SSB / df_B (between-group degrees of freedom).\n",
        "2. MS Within (MSW): MSW = SSW / df_W (within-group degrees of freedom).\n",
        "\n",
        "F-Statistic Calculation\n",
        "\n",
        "1. F = MSB / MSW: Ratio of between-group variance to within-group variance.\n",
        "2. F measures significance: Large F values indicate significant differences between groups.\n",
        "\n",
        "Degrees of Freedom\n",
        "\n",
        "1. df_B = k - 1 (number of groups minus 1).\n",
        "2. df_W = N - k (total observations minus number of groups).\n",
        "\n",
        "Partitioning Contribution\n",
        "\n",
        "1. Variance partitioning: Separates systematic differences (SSB) from random error (SSW).\n",
        "2. F-statistic calculation: MSB and MSW are used to calculate the F-statistic.\n",
        "3. Significance testing: F-statistic determines whether group differences are statistically significant.\n",
        "\n",
        "Example\n",
        "\n",
        "Suppose we have three groups (A, B, C) with means 10, 15, and 20, and overall mean 15.\n",
        "\n",
        "| Group | Mean | n |\n",
        "| --- | --- | --- |\n",
        "| A    | 10  | 5 |\n",
        "| B    | 15  | 5 |\n",
        "| C    | 20  | 5 |\n",
        "\n",
        "SSB = 5(10-15)^2 + 5(15-15)^2 + 5(20-15)^2 = 250\n",
        "SSW = ΣΣ(x_ij - μ_i)^2 = 50\n",
        "SST = SSB + SSW = 300\n",
        "MSB = SSB / 2 = 125\n",
        "MSW = SSW / 12 = 4.17\n",
        "F = MSB / MSW = 125 / 4.17 ≈ 30"
      ],
      "metadata": {
        "id": "-HCS3O9LiYkT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key\n",
        "differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?\n",
        "\n",
        "ANS--> Classical (Frequentist) ANOVA and Bayesian ANOVA differ fundamentally in handling uncertainty, parameter estimation, and hypothesis testing:\n",
        "\n",
        "Uncertainty Handling\n",
        "\n",
        "1. Frequentist: Uncertainty is represented through probability distributions of test statistics, assuming the null hypothesis is true (p-values).\n",
        "2. Bayesian: Uncertainty is represented through probability distributions of model parameters (posterior distributions), incorporating prior knowledge and data.\n",
        "\n",
        "Parameter Estimation\n",
        "\n",
        "1. Frequentist: Point estimates (e.g., least squares) and confidence intervals for population parameters.\n",
        "2. Bayesian: Posterior distributions for model parameters, providing distributional estimates.\n",
        "\n",
        "Hypothesis Testing\n",
        "\n",
        "1. Frequentist: Null hypothesis significance testing (NHST), comparing p-values to alpha levels.\n",
        "2. Bayesian: Bayesian hypothesis testing, comparing posterior probabilities of competing models or parameter values.\n",
        "\n",
        "Additional Key Differences\n",
        "\n",
        "1. Prior knowledge: Bayesian ANOVA incorporates prior distributions for model parameters, while frequentist ANOVA does not.\n",
        "2. Model comparison: Bayesian ANOVA facilitates model comparison through Bayes factors or posterior model probabilities.\n",
        "3. Interpretation: Frequentist p-values indicate statistical significance, whereas Bayesian results provide probabilistic statements about parameter values.\n",
        "4. Computational methods: Bayesian ANOVA often requires Markov Chain Monte Carlo (MCMC) simulations or variational inference.\n",
        "\n",
        "Bayesian ANOVA Advantages\n",
        "\n",
        "1. Improved interpretation: Probabilistic statements about parameter values.\n",
        "2. Incorporating prior knowledge: Informative priors can improve estimation.\n",
        "3. Model comparison: Bayesian methods facilitate model selection.\n",
        "4. Flexibility: Bayesian ANOVA can handle non-normal data and complex models.\n",
        "\n",
        "Frequentist ANOVA Advantages\n",
        "\n",
        "1. Computational efficiency: Faster computation compared to Bayesian methods.\n",
        "2. Established methodology: Well-developed frequentist ANOVA procedures.\n",
        "3. Easy interpretation: P-values provide straightforward significance assessments.\n",
        "\n",
        "Software Implementation\n",
        "\n",
        "1. Frequentist ANOVA: R (aov()), Python (statsmodels), SAS (PROC ANOVA)\n",
        "2. Bayesian ANOVA: R (brms, BayesFactor), Python (PyMC3, BayesianRegression), Stan\n",
        "\n",
        "Common Bayesian ANOVA Models\n",
        "\n",
        "1. Linear Bayesian ANOVA: Extensions of classical ANOVA.\n",
        "2. Hierarchical Bayesian ANOVA: Incorporating random effects.\n",
        "3. Non-parametric Bayesian ANOVA: Relaxing distributional assumptions.\n",
        "\n",
        "Best Practices\n",
        "\n",
        "1. Choose appropriate approach: Consider research question, data characteristics, and computational resources.\n",
        "2. Specify informative priors: When using Bayesian ANOVA.\n",
        "3. Report Bayesian results: Include posterior distributions, Bayes factors, or posterior model probabilities.\n",
        "4. Interpret results carefully: Consider context, model assumptions, and limitations."
      ],
      "metadata": {
        "id": "43NDlpzOiwmz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. Question: You have two sets of data representing the incomes of two different professions1\n",
        "* Profession A: [48, 52, 55, 60, 62]\n",
        "* Profession B: [45, 50, 55, 52, 47] Perform an F-test to determine if the variances of the two professions'\n",
        "incomes are equal. What are your conclusions based on the F-test?\n",
        "\n",
        "Task: Use Python to calculate the F-statistic and p-value for the given data.\n",
        "\n",
        "Objective: Gain experience in performing F-tests and interpreting the results in terms of variance comparison."
      ],
      "metadata": {
        "id": "ch1CXrICjGNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import f\n",
        "\n",
        "# Income data for Profession A and Profession B\n",
        "profession_a = np.array([48, 52, 55, 60, 62])\n",
        "profession_b = np.array([45, 50, 55, 52, 47])\n",
        "\n",
        "# Calculate variances\n",
        "variance_a = np.var(profession_a, ddof=1)  # Use ddof=1 for sample variance\n",
        "variance_b = np.var(profession_b, ddof=1)\n",
        "\n",
        "# Calculate F-statistic\n",
        "f_statistic = variance_a / variance_b\n",
        "\n",
        "# Degrees of freedom\n",
        "df1 = len(profession_a) - 1\n",
        "df2 = len(profession_b) - 1\n",
        "\n",
        "# Calculate p-value\n",
        "p_value = 2 * min(f.cdf(f_statistic, df1, df2), 1 - f.cdf(f_statistic, df1, df2)) #Two-tailed test\n",
        "\n",
        "print(f\"F-statistic: {f_statistic:.2f}\")\n",
        "print(f\"P-value: {p_value:.3f}\")\n",
        "\n",
        "# Interpret the results\n",
        "alpha = 0.05  # Significance level\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: Variances are significantly different.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: Variances are not significantly different.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekfygxeQjcaI",
        "outputId": "c34b8cc7-20b5-4e57-988c-46b1ea38094c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 2.09\n",
            "P-value: 0.493\n",
            "Fail to reject the null hypothesis: Variances are not significantly different.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. Question: Conduct a one-way ANOVA to test whether there are any statistically significant differences in\n",
        "average heights between three different regions with the following data1\n",
        "* Region A: [160, 162, 165, 158, 164'\n",
        "* Region B: [172, 175, 170, 168, 174'\n",
        "* Region C: [180, 182, 179, 185, 183'\n",
        "* Task: Write Python code to perform the one-way ANOVA and interpret the results.\n",
        "* Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value."
      ],
      "metadata": {
        "id": "VM6S7GDvju3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "\n",
        "# Height data for three regions\n",
        "region_a = [160, 162, 165, 158, 164]\n",
        "region_b = [172, 175, 170, 168, 174]\n",
        "region_c = [180, 182, 179, 185, 183]\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "f_statistic, p_value = stats.f_oneway(region_a, region_b, region_c)\n",
        "\n",
        "print(f\"F-statistic: {f_statistic:.2f}\")\n",
        "print(f\"P-value: {p_value:.3f}\")\n",
        "\n",
        "# Interpret the results\n",
        "alpha = 0.05  # Significance level\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: There are significant differences in average heights between the regions.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: There are no significant differences in average heights between the regions.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwKGBuo_l2uf",
        "outputId": "421d3c89-fa1f-4c7f-f886-aeee61cd0e76"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 67.87\n",
            "P-value: 0.000\n",
            "Reject the null hypothesis: There are significant differences in average heights between the regions.\n"
          ]
        }
      ]
    }
  ]
}